{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import h5py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import roboticstoolbox as rtb\n",
    "import json\n",
    "import open3d as o3d\n",
    "from multiprocessing.shared_memory import SharedMemory\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import pyquaternion as pyq  \n",
    "from scipy.optimize import minimize\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def wait_for_data(width, height, control_data:np.ndarray, data_shm:SharedMemory):\n",
    "    while True:\n",
    "        try:\n",
    "            if control_data[0] <= 0.5:\n",
    "                buffer = np.ndarray((height, width, 3), dtype=np.uint8, buffer=data_shm.buf)\n",
    "                image = np.array(buffer)\n",
    "                return image\n",
    "        except Exception as e:\n",
    "            print(control_data)\n",
    "            raise e\n",
    "\n",
    "def send_by_shm(control_data:np.ndarray, cam_pose, fx, fy, width, height):\n",
    "    cam_trans, cam_rot = cam_pose\n",
    "    control_data[1:4] = cam_trans\n",
    "    control_data[4:8] = cam_rot\n",
    "    control_data[8] = fx\n",
    "    control_data[9] = fy\n",
    "    control_data[10] = width\n",
    "    control_data[11] = height\n",
    "    control_data[0] = 1.0\n",
    "    \n",
    "def get_rendered_images(cam_pose, height, width, fx, fy):\n",
    "    control_shm = SharedMemory(name='control_psm_08d5dd701')\n",
    "    data_shm = SharedMemory(name=\"data_psm_08d5dd701\")\n",
    "    control_data = np.ndarray((12,), dtype=np.float64, buffer=control_shm.buf)\n",
    "    position = cam_pose[0]\n",
    "    rotation = cam_pose[1]\n",
    "    send_by_shm(control_data, cam_pose, fx, fy, width, height)\n",
    "    image = wait_for_data(width, height, control_data, data_shm)\n",
    "    return image\n",
    "\n",
    "def get_to_marker_pose(image, camera_params, tag_size, detector, min_num=4):\n",
    "    '''\n",
    "    get the pose: camera -> marker\n",
    "    '''\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    results = detector.detect(gray, estimate_tag_pose=True, camera_params=camera_params, tag_size=tag_size)\n",
    "    if len(results) < min_num:\n",
    "        return None\n",
    "    res = None\n",
    "    for result in results:\n",
    "        if result.tag_id == 22222:\n",
    "            res = result\n",
    "            break\n",
    "    if res is None:\n",
    "        return None\n",
    "    pose = np.eye(4)\n",
    "    pose[:3, :3] = res.pose_R\n",
    "    pose[:3, 3] = res.pose_t[:, 0]\n",
    "    pose = np.linalg.inv(pose)\n",
    "    hope2now = np.eye(4)\n",
    "    hope2now[1, 1] = -1\n",
    "    hope2now[2, 2] = -1\n",
    "    now2hope = np.linalg.inv(hope2now)\n",
    "    tmp_trans = np.eye(4)\n",
    "    tmp_trans[2, 2] = -1\n",
    "    pose = now2hope @ pose \n",
    "    return pose\n",
    "\n",
    "def create_camera_model(size=0.1):\n",
    "    mesh_camera = o3d.geometry.TriangleMesh.create_coordinate_frame(size=size, origin=[0, 0, 0])\n",
    "    # mesh_camera.paint_uniform_color([0.9, 0.1, 0.1])\n",
    "    return mesh_camera\n",
    "\n",
    "def show_pose(camera_pose, size=0.1):\n",
    "    camera_pose = np.array(camera_pose)\n",
    "    camera_pose[:3, :3] = camera_pose[:3, :3] / np.abs((np.linalg.det(camera_pose[:3, :3]))) ** (1/3)\n",
    "    tmp_tans = np.eye(4)\n",
    "    tmp_tans[2, 2] = -1\n",
    "    # camera_pose =  camera_pose @ tmp_tans\n",
    "    camera_model = create_camera_model(size)\n",
    "    camera_model.transform(camera_pose)\n",
    "    return camera_model\n",
    "\n",
    "def filter_and_compute_mean_std(matrices, outlier_percent=0.1):\n",
    "    \"\"\"\n",
    "    Filter out a given percentage of outlier matrices and compute the mean and standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    matrices (numpy.ndarray): A numpy array of shape (n, m, m) representing n m x m matrices.\n",
    "    outlier_percent (float): The percentage of outliers to remove (0 < outlier_percent < 0.5), default is to remove the top and bottom 10%.\n",
    "\n",
    "    Returns:\n",
    "    mean_matrix (numpy.ndarray): The mean matrix after filtering.\n",
    "    std_matrix (numpy.ndarray): The standard deviation matrix after filtering.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the norm of each matrix\n",
    "    norms = np.linalg.norm(matrices, axis=(1, 2))\n",
    "\n",
    "    # Step 2: Sort the norms\n",
    "    sorted_indices = np.argsort(norms)\n",
    "\n",
    "    # Step 3: Compute the bounds for removing outliers\n",
    "    n_matrices = len(matrices)\n",
    "    lower_bound = int(n_matrices * outlier_percent)\n",
    "    upper_bound = int(n_matrices * (1 - outlier_percent))\n",
    "\n",
    "    # Step 4: Filter matrices\n",
    "    filtered_indices = sorted_indices[lower_bound:upper_bound]\n",
    "    filtered_matrices = matrices[filtered_indices]\n",
    "\n",
    "    # Step 5: Compute the mean and standard deviation after removing outliers\n",
    "    mean_matrix = np.mean(filtered_matrices, axis=0)\n",
    "    std_matrix = np.std(filtered_matrices, axis=0)\n",
    "\n",
    "    return mean_matrix, std_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = \"/home/pjlab/.local/share/ov/pkg/isaac-sim-4.0.0/src/frankapy/logs/2024-09-22_17-14-52/log-000001-8404\"\n",
    "log_folder = \"/home/pjlab/.local/share/ov/pkg/isaac-sim-4.0.0/src/frankapy/logs/2024-09-22_17-52-45/log-000002-5588\"\n",
    "calibration_eyeinhand_results_dir = \"/home/pjlab/Franka_Tool/catkin_ws_2/src/easy_handeye/results/eye_on_hand/9-21--15-38.json\"\n",
    "gs_dir = \"/home/pjlab/main/real2sim/gaussian-splatting/data/new/mix2/gs-output/1\"\n",
    "video_output_path = \"/home/pjlab/.local/share/ov/pkg/isaac-sim-4.0.0/src/frankapy/tests/test-render/output_video923-6.mp4\"\n",
    "marker_2_base = np.load(\"/home/pjlab/.local/share/ov/pkg/isaac-sim-4.0.0/src/assets/marker_2_base.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fx, fy, width, height: 604.0827026367188 602.91064453125 640 480\n",
      "cam_EE:\n",
      "[[ 4.68582381e-04 -9.98368542e-01 -5.70967069e-02  8.01850673e-02]\n",
      " [ 9.99527826e-01  2.22178170e-03 -3.06461778e-02 -2.74386378e-02]\n",
      " [ 3.07230363e-02 -5.70553871e-02  9.97898180e-01 -8.94739605e-02]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "Total num: 2330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "from pupil_apriltags import Detector\n",
    "import os\n",
    "\n",
    "os.environ[\"DISPLAY\"] = \":1\"\n",
    "\n",
    "# file path\n",
    "log_folder = Path(log_folder)\n",
    "hdf5_file = log_folder / \"traj.hdf5\"\n",
    "with open(calibration_eyeinhand_results_dir, 'r') as f:\n",
    "    calibration_results = json.load(f)\n",
    "\n",
    "hdf5_file = h5py.File(hdf5_file, 'r')\n",
    "joints = hdf5_file['joints'][:]\n",
    "timestamps = hdf5_file[\"timestamp\"][:]\n",
    "fx, fy = hdf5_file[\"intrinsic_color_1\"][\"fx\"][()], hdf5_file[\"intrinsic_color_1\"][\"fy\"][()]\n",
    "image_width, image_height = hdf5_file[\"intrinsic_color_1\"][\"width\"][()], hdf5_file[\"intrinsic_color_1\"][\"height\"][()]\n",
    "print(\"fx, fy, width, height:\", fx, fy, image_width, image_height)\n",
    "qposes = joints[:, :7]\n",
    "\n",
    "cam_2_ee = np.eye(4)\n",
    "quat = np.array([calibration_results[\"rotation\"][\"x\"], calibration_results[\"rotation\"][\"y\"], calibration_results[\"rotation\"][\"z\"], calibration_results[\"rotation\"][\"w\"]])\n",
    "cam_2_ee[:3, :3] = R.from_quat(quat).as_matrix()\n",
    "cam_2_ee[:3, 3] = np.array([calibration_results[\"translation\"][\"x\"], calibration_results[\"translation\"][\"y\"], calibration_results[\"translation\"][\"z\"]])\n",
    "print(f\"cam_EE:\\n{cam_2_ee}\")\n",
    "\n",
    "detector =  Detector(families=\"tagStandard52h13\",\n",
    "                    nthreads=1,\n",
    "                    quad_decimate=1.0,\n",
    "                    quad_sigma=0.0,\n",
    "                    refine_edges=1,\n",
    "                    decode_sharpening=0.25,\n",
    "                    debug=0)\n",
    "\n",
    "franka = rtb.models.Panda()\n",
    "item_to_show = []\n",
    "marker_2_bases = []\n",
    "print(\"Total num:\", len(qposes))\n",
    "# Define video save path and parameters\n",
    "frame_rate = 30  # Set frame rate, adjust as needed\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')  # Use MP4V encoding\n",
    "\n",
    "# Initialize VideoWriter object\n",
    "# Assuming all frames are the same size, use the width and height of the first frame\n",
    "first_frame = True\n",
    "video_writer = None\n",
    "item_to_show = []\n",
    "gs_2_marker = np.load(Path(gs_dir) / \"gs_to_marker.npy\")\n",
    "# Temporary alignment adjustments\n",
    "marker_2_base[2, 3] -= 0.1\n",
    "marker_2_base[1, 3] -= 0.036  # Move to the right\n",
    "marker_2_base[0, 3] += 0.02  # Move forward\n",
    "\n",
    "for qpos, timestamp in zip(qposes, timestamps):\n",
    "    ee_2_base = np.array(franka.fkine(qpos))\n",
    "    cam_2_base = ee_2_base @ cam_2_ee @ rtb.ET.Rz(-np.pi/2).A()\n",
    "    cam_2_marker = np.linalg.inv(marker_2_base) @ cam_2_base @ rtb.ET.Rx(-np.pi/2).A() @ rtb.ET.Rz(-np.pi/2).A() @ rtb.ET.Rx(-np.pi / 2).A()\n",
    "    item_to_show.append(show_pose(cam_2_marker, 0.1))\n",
    "    wrist_camera_pose = np.linalg.inv(gs_2_marker) @ rtb.ET.Rz(np.pi / 2).A() @ cam_2_marker \n",
    "    rotation_matrix = wrist_camera_pose[:3, :3] / np.abs((np.linalg.det(wrist_camera_pose[:3, :3]))) ** (1/3)\n",
    "    # Apply camera pose transformation\n",
    "    translation = wrist_camera_pose[:3, 3]\n",
    "    x, y, z, w = R.from_matrix(rotation_matrix).as_quat()\n",
    "    wrist_camera_pose = [translation, [w, x, y, z]]\n",
    "\n",
    "    # If it's the first frame, initialize VideoWriter\n",
    "    if first_frame:\n",
    "        video_writer = cv2.VideoWriter(\n",
    "            video_output_path, \n",
    "            fourcc, \n",
    "            frame_rate, \n",
    "            (image_width, image_height)\n",
    "        )\n",
    "        first_frame = False\n",
    "\n",
    "    # Generate image\n",
    "    image = get_rendered_images(\n",
    "        wrist_camera_pose,\n",
    "        image_height,\n",
    "        image_width,\n",
    "        fx, \n",
    "        fy\n",
    "    )\n",
    "\n",
    "    # Ensure the image is in BGR format\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Write frame to video\n",
    "    video_writer.write(image)\n",
    "video_writer.release()\n",
    "frame_base = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])\n",
    "item_to_show.append(frame_base)\n",
    "o3d.visualization.draw_geometries(item_to_show)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
